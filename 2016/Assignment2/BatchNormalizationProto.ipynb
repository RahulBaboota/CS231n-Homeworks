{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Forward Pass </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the different inputs required for the batch normalization forward pass.\n",
    "\n",
    "## Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "X = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "## Setting gamma = 1 and beta = 0.\n",
    "gamma = np.ones(D3)\n",
    "beta = np.ones(D3)\n",
    "\n",
    "## Additional parameters.\n",
    "batchNormDict = {'mode' : 'train',\n",
    "                 'eps' : 1e-5,\n",
    "                 'momentum' : 0.9,\n",
    "                 'runningMean' : np.zeros(D3),\n",
    "                 'runningVar' : np.zeros(D3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the forward pass.\n",
    "## We compute the forward pass as a computational graph so as to easily \n",
    "## backpropagate into the network. \n",
    "\n",
    "## Computing the mean of the sample.\n",
    "sampleMean = (1.0/N) * (np.sum(X, axis = 0))\n",
    "\n",
    "## Computing the numerator expression (X - E[X]).\n",
    "numExpression = X - sampleMean\n",
    "\n",
    "## Computing the denominator expression (Standard Deviation of X).\n",
    "interMediate = numExpression ** 2\n",
    "varianceInput = (1.0/N) * (np.sum(interMediate, axis = 0))\n",
    "stableSD = np.sqrt(varianceInput + batchNormDict['eps'])\n",
    "\n",
    "## Inverting the standard deviation.\n",
    "invertedSD = (1.0/stableSD)\n",
    "\n",
    "## Computing the normalised gaussian input.\n",
    "xHat = numExpression * invertedSD\n",
    "\n",
    "## Scaling the normalised gaussian input by gamma.\n",
    "xHatScaled = gamma * xHat\n",
    "\n",
    "## Shifting the normalised gaussian input by beta.\n",
    "xHatShifted = xHatScaled + beta\n",
    "out = xHatShifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the mean and variance of the input along each dimension (feature).\n",
    "sampleMean = np.mean(out, axis = 0)\n",
    "sampleVariance = np.var(out, axis = 0)\n",
    "\n",
    "## Normalizing the input.\n",
    "out = ((out - sampleMean)/sampleVariance)\n",
    "\n",
    "## Scaling and Shifting the normalized data.\n",
    "out = (gamma * out + beta)\n",
    "\n",
    "## Updating the running mean and running variance.\n",
    "batchNormDict['runningMean'] = batchNormDict['momentum'] * batchNormDict['runningMean'] + (1 - batchNormDict['momentum']) * sampleMean\n",
    "batchNormDict['runningVar'] = batchNormDict['momentum'] * batchNormDict['runningVar'] + (1 - batchNormDict['momentum']) * sampleVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Backward Pass </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation Parameters.\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dOut = np.random.randn(N, D)\n",
    "bn_param = {'mode': 'train'}\n",
    "\n",
    "## Computing the forward pass.\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "out, xHatShifted, xHatScaled, xHat, invertedSD, stableSD, sampleVariance, interMediate, numExpression, sampleMean, gamma, beta, eps = cache\n",
    "\n",
    "## Defining a function to compute the output so that it's numerical gradient can be evaluated for gradient checking.\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "## Computing the numerical gradients for performing sanity checks.\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dOut)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dOut)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the backward pass.\n",
    "\n",
    "## Computing the gradient with respect to the beta parameter.\n",
    "dBeta = np.sum(dOut, axis = 0)\n",
    "\n",
    "## Computing the gradient with respect to the gamma parameter.\n",
    "dGamma = np.sum(xHat * dOut, axis = 0)\n",
    "\n",
    "## Computing the gradient with respect to xHat.\n",
    "dXhat = (gamma * dOut)\n",
    "\n",
    "## Computing the gradient with respect to inverted standard deviation.\n",
    "dInvertedSD = np.sum( numExpression * dXhat, axis = 0)\n",
    "\n",
    "## Computing the gradient with respect to the numerator expression (P1).\n",
    "dNumExpressionP1 = (invertedSD * dXhat)\n",
    "\n",
    "## Computing the gradient with respect to the standard deviation.\n",
    "dStableSD = ((-1.0) * (invertedSD**2) * dInvertedSD)\n",
    "\n",
    "## Computing the gradient with respect to the sample variance.\n",
    "dSampleVariance = ((0.5) * (1.0 / np.sqrt(sampleVariance + eps)) * dStableSD)\n",
    "\n",
    "## Computing the gradient with respect to the interMediate.\n",
    "dInterMediate = ((1.0 / N) * np.ones((N, D)) * dSampleVariance)\n",
    "\n",
    "## Computing the gradient with respect to the numerator expression (P2).\n",
    "dNumExpressionP2 = ((2.0) * numExpression * dInterMediate)\n",
    "\n",
    "## Combining the gradients to obtain the full gradient with respect to the numerator expression.\n",
    "dNumExpression = dNumExpressionP1  + dNumExpressionP2\n",
    "\n",
    "## Computing the gradient with respect to the sample mean.\n",
    "dSampleMean = (-1) * np.sum(dNumExpression, axis = 0)\n",
    "\n",
    "## Computing the gradient with respect to the input.\n",
    "dXP1 = ((1.0 / N) * np.ones((N, D)) * dSampleMean)\n",
    "dXP2 = dNumExpression\n",
    "dx = dXP1 + dXP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbeta error:  5.53012302586e-11\n",
      "dgamma error:  9.97382936252e-12\n",
      "dx error:  1.89593804747e-09\n"
     ]
    }
   ],
   "source": [
    "print 'dbeta error: ', rel_error(db_num, dBeta)\n",
    "print 'dgamma error: ', rel_error(da_num, dGamma)\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
