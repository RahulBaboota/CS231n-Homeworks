{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Basic Setup </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiating the argument values.\n",
    "inputSize = 32 * 32 * 3\n",
    "hiddenSize = 50\n",
    "outputSize = 10\n",
    "std = 1e-4\n",
    "\n",
    "## Dictionary to hold the values of the network parameters.\n",
    "\n",
    "params = {}\n",
    "params['W1'] = std * np.random.randn(inputSize, hiddenSize)\n",
    "params['b1'] = np.zeros(hiddenSize)\n",
    "params['W2'] = std * np.random.randn(hiddenSize, outputSize)\n",
    "params['b2'] = np.zeros(outputSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Computing Scores </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1 = params['W1'], params['b1']\n",
    "W2, b2 = params['W2'], params['b2']\n",
    "\n",
    "N, D = X_val.shape\n",
    "reg = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = None\n",
    "\n",
    "## Computing the first hidden layer.\n",
    "hiddenLayer1 = X_val.dot(W1) + b1\n",
    "\n",
    "## Applying Relu to the hidden layer.\n",
    "activatedHiddenLayer1 = np.clip(hiddenLayer1, 0, None)\n",
    "\n",
    "## Computing the second hidden layer.\n",
    "hiddenLayer2 = activatedHiddenLayer1.dot(W2) + b2\n",
    "\n",
    "## Storing this matrix in the scores variable.\n",
    "scores = hiddenLayer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302951688604538"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalise the raw scores to avoid exponential score blow-up.\n",
    "## To do so, subtract the maximum score from each score value for each image.\n",
    "expScores = np.exp(scores - np.max(scores, axis = 1, keepdims = True))\n",
    "\n",
    "## Compute the probabilities (or softmax scores) of each class.\n",
    "softmaxScores = expScores/np.sum(expScores, axis = 1, keepdims = True)\n",
    "\n",
    "## Creating a 1-D matrix containing the softmax score of the correct class.\n",
    "corrSoftScore = np.choose(y_val, softmaxScores.T)\n",
    "\n",
    "## Computing the cross-entropy loss.\n",
    "loss = -np.sum(np.log(corrSoftScore), axis = 0, keepdims = True)\n",
    "\n",
    "## Extracting the single float value from the 1 element numpy array.\n",
    "loss = loss[0]\n",
    "\n",
    "## Compute the full training loss by dividing the cummulative loss by the number of training instances.\n",
    "loss = loss/N\n",
    "\n",
    "## Add regularisation loss.\n",
    "loss = loss + 0.5 * reg * np.sum(W1 * W1) + 0.5 * reg * np.sum(W2 * W2)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = {}\n",
    "\n",
    "# Backward pass: compute gradients\n",
    "dO = softmaxScores\n",
    "\n",
    "## Computing dL/dO (Softmax Gradient).\n",
    "dO[np.arange(N), y_val] -= 1\n",
    "dO /= N\n",
    "\n",
    "## Computing dL/db2.\n",
    "grads['b2'] = np.sum(dO, axis = 0)\n",
    "\n",
    "## Computing dL/dW2.\n",
    "grads['W2']= activatedHiddenLayer1.T.dot(dO) + reg * W2\n",
    "\n",
    "## Computing dL/dActivatedHiddenLayer1.\n",
    "dActivatedHiddenLayer1 = dO.dot(W2.T)\n",
    "\n",
    "## Computing dL/dHiddenLayer1 (Backprop through Relu).\n",
    "dActivatedHiddenLayer1[activatedHiddenLayer1 <= 0] = 0\n",
    "# dHiddenLayer1 = np.clip(dActivatedHiddenLayer1, 0, None)\n",
    "\n",
    "## Computing dL/db1.\n",
    "grads['b1'] = np.sum(dActivatedHiddenLayer1, axis = 0)\n",
    "\n",
    "## Computing dL/dW1.\n",
    "grads['W1'] = X_val.T.dot(dActivatedHiddenLayer1) + reg * W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = None\n",
    "reLU = lambda x: np.maximum(0, x)\n",
    "hidden_layer = reLU(X_val.dot(W1) + b1)\n",
    "scores = hidden_layer.dot(W2) + b2\n",
    "\n",
    "loss = None\n",
    "scores -= np.max(scores)\n",
    "probs = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "loss = np.mean(-np.log(probs[np.arange(N), y_val]))\n",
    "loss += 0.5 * reg * (np.sum(W1**2) + np.sum(W2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads2 = {}\n",
    "dscores = probs\n",
    "dscores[np.arange(N), y_val] -= 1\n",
    "dscores /= N\n",
    "\n",
    "grads2['W2'] = hidden_layer.T.dot(dscores)\n",
    "grads2['b2'] = np.sum(dscores, axis=0)\n",
    "dhidden = dscores.dot(W2.T)\n",
    "dhidden[hidden_layer <= 0] = 0\n",
    "grads2['W1'] = X_val.T.dot(dhidden)\n",
    "grads2['b1'] = np.sum(dhidden, axis=0)\n",
    "\n",
    "grads2['W1'] += reg * W1\n",
    "grads2['W2'] += reg * W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhidden[hidden_layer < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
